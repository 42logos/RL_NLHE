train_settings:
  algo: PPO
  epochs: 150
  batch_size: 2048
  num_mini_batches: 2048
  learning_rate: 3e-4
  num_epochs: 10
  clip_grad_norm: 77
  gamma: 0.8
  shuffle_batches: true
  num_env_runners: 12

network_settings: 
  fc_hidden_sizes: [256, 256]
  fc_activation: relu
  head_hidden_sizes: [128]
  head_activation: relu
  use_lstm: false
  lstm_hidden_size: 128
  free_log_std: false

env_settings:
  name: nlhe
  hero_seat: 1
  bb: 2
  sb: 1
  seed: 42
  starting_stack: 100
  history_length: 64
  


eval_settings:
  interval: 2            # evaluate every 2 train() calls
  num_episodes: 64       # episodes per evaluation
  deterministic: false    # explore=False during eval
  seed: 424242           # fixed seed for reproducibility
  parallel: false        # run eval after training step (no lag)
  num_env_runners: 1     # single runner for strict repeatability

log_settings:
  tensorboard:
    log_dir: ./tb_logs
    # If you want to override/limit the whitelist explicitly:
    allow:
      - "env_runners/episode_return_mean"
      - "env_runners/episode_return_max"
      - "env_runners/episode_return_min"
      - "env_runners/episode_len_mean"
      - "env_runners/episode_len_max"
      - "env_runners/episode_len_min"
      - "learners/default_policy/mean_kl_loss"
      - "learners/default_policy/policy_loss"
      - "learners/default_policy/vf_loss"
      - "learners/default_policy/vf_loss_unclipped"
      - "learners/default_policy/total_loss"
      - "evaluation/env_runners/episode_return_max"
      - "evaluation/env_runners/episode_return_mean"
      - "evaluation/env_runners/episode_return_min"

    tag_map:
      "env_runners/episode_return_mean": "train/return_mean"
      "evaluation/env_runners/episode_return_mean": "eval/return_mean"
      "learners/default_policy/policy_loss": "loss/policy"
      "learners/default_policy/vf_loss": "loss/vf"
      "learners/default_policy/vf_loss_unclipped": "loss/vf_unclipped"
      "learners/default_policy/total_loss": "loss/total"
      "learners/default_policy/mean_kl_loss": "loss/mean_kl"
      "evaluation/return_mean": "eval/return_mean"
      "evaluation/return_max": "eval/return_max"
      "evaluation/return_min": "eval/return_min"
      "env_runners/episode_return_max": "train/return_max"
      "env_runners/episode_return_min": "train/return_min"
  checkpointing:
    save_dir: ./checkpoints
    save_freq: 2
    max_to_keep: 5

# ====== Optuna Sweeper 设置 ======

defaults:
  - override hydra/sweeper: optuna


hydra:
  sweeper:
    direction: maximize               # 让训练脚本返回“越大越好”的指标（如 EPReturn）
    storage: sqlite:///optuna_study.db
    study_name: core_tune_v1
    n_trials: 60
    n_jobs: 1                         # 单机单卡建议 1；多卡或CPU可调高


    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 2025
      n_startup_trials: 10

    # ====== 搜索空间（关键） ======
    params:
      # 批大小：从 1024 到 9600，步进 512（可按你数据管线粒度调整步长）
      train_settings.batch_size: range(1024, 9600)

      # 学习率：对数尺度的连续区间
      train_settings.learning_rate: tag(log, interval(1e-5, 1e-2))

      # 梯度范数裁剪阈值
      train_settings.clip_grad_norm: interval(0.5, 8.0)

      # 折扣因子
      train_settings.gamma: interval(0.80, 0.999)

      # LSTM 相关
      network_settings.use_lstm: choice(true, false)
      network_settings.lstm_hidden_size: choice(128, 256)